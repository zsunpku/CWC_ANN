{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your first Edward program\n",
    "\n",
    "### note this is just copied from edward: see https://github.com/blei-lab/edward/blob/master/notebooks/getting_started.ipynb ###\n",
    "\n",
    "Probabilistic modeling in Edward uses a simple language of random variables. Here we will show a Bayesian neural network. It is a neural network with a prior distribution on its weights.\n",
    "\n",
    "A webpage version is available at \n",
    "http://edwardlib.org/getting-started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import edward as ed\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from edward.models import Normal\n",
    "\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_toy_dataset(N=50, noise_std=0.1):\n",
    "  x = np.linspace(-3, 3, num=N)\n",
    "  y = np.cos(x) + np.random.normal(0, noise_std, size=N)\n",
    "  x = x.astype(np.float32).reshape((N, 1))\n",
    "  y = y.astype(np.float32)\n",
    "  return x, y\n",
    "\n",
    "\n",
    "def neural_network(x, W_0, W_1, b_0, b_1):\n",
    "  h = tf.tanh(tf.matmul(x, W_0) + b_0)\n",
    "  h = tf.matmul(h, W_1) + b_1\n",
    "  return tf.reshape(h, [-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, simulate a toy dataset of 50 observations with a cosine relationship."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ed.set_seed(42)\n",
    "\n",
    "N = 50  # number of data ponts\n",
    "D = 1   # number of features\n",
    "\n",
    "x_train, y_train = build_toy_dataset(N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, define a two-layer Bayesian neural network. Here, we define the neural network manually with `tanh` nonlinearities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W_0 = Normal(loc=tf.zeros([D, 2]), scale=tf.ones([D, 2]))\n",
    "W_1 = Normal(loc=tf.zeros([2, 1]), scale=tf.ones([2, 1]))\n",
    "b_0 = Normal(loc=tf.zeros(2), scale=tf.ones(2))\n",
    "b_1 = Normal(loc=tf.zeros(1), scale=tf.ones(1))\n",
    "\n",
    "x = x_train\n",
    "y = Normal(loc=neural_network(x, W_0, W_1, b_0, b_1),\n",
    "           scale=0.1 * tf.ones(N))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, make inferences about the model from data. We will use variational inference. Specify a normal approximation over the weights and biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "qW_0 = Normal(loc=tf.Variable(tf.random_normal([D, 2])),\n",
    "              scale=tf.nn.softplus(tf.Variable(tf.random_normal([D, 2]))))\n",
    "qW_1 = Normal(loc=tf.Variable(tf.random_normal([2, 1])),\n",
    "              scale=tf.nn.softplus(tf.Variable(tf.random_normal([2, 1]))))\n",
    "qb_0 = Normal(loc=tf.Variable(tf.random_normal([2])),\n",
    "              scale=tf.nn.softplus(tf.Variable(tf.random_normal([2]))))\n",
    "qb_1 = Normal(loc=tf.Variable(tf.random_normal([1])),\n",
    "              scale=tf.nn.softplus(tf.Variable(tf.random_normal([1]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining `tf.Variable` allows the variational factors’ parameters to vary. They are initialized randomly. The standard deviation parameters are constrained to be greater than zero according to a [softplus](https://en.wikipedia.org/wiki/Rectifier_(neural_networks)) transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sample functions from variational model to visualize fits.\n",
    "rs = np.random.RandomState(0)\n",
    "inputs = np.linspace(-5, 5, num=400, dtype=np.float32)\n",
    "x = tf.expand_dims(inputs, 1)\n",
    "mus = tf.stack(\n",
    "    [neural_network(x, qW_0.sample(), qW_1.sample(),\n",
    "                    qb_0.sample(), qb_1.sample())\n",
    "     for _ in range(10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# FIRST VISUALIZATION (prior)\n",
    "\n",
    "sess = ed.get_session()\n",
    "tf.global_variables_initializer().run()\n",
    "outputs = mus.eval()\n",
    "\n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.set_title(\"Iteration: 0\")\n",
    "ax.plot(x_train, y_train, 'ks', alpha=0.5, label='(x, y)')\n",
    "ax.plot(inputs, outputs[0].T, 'r', lw=2, alpha=0.5, label='prior draws')\n",
    "ax.plot(inputs, outputs[1:].T, 'r', lw=2, alpha=0.5)\n",
    "ax.set_xlim([-5, 5])\n",
    "ax.set_ylim([-2, 2])\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, run variational inference with the [Kullback-Leibler](https://en.wikipedia.org/wiki/Kullback–Leibler_divergence) divergence in order to infer the model’s latent variables with the given data. We specify `1000` iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference = ed.KLqp({W_0: qW_0, b_0: qb_0,\n",
    "                     W_1: qW_1, b_1: qb_1}, data={y: y_train})\n",
    "inference.run(n_iter=1000, n_samples=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, criticize the model fit. Bayesian neural networks define a distribution over neural networks, so we can perform a graphical check. Draw neural networks from the inferred model and visualize how well it fits the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SECOND VISUALIZATION (posterior)\n",
    "\n",
    "outputs = mus.eval()\n",
    "\n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.set_title(\"Iteration: 1000\")\n",
    "ax.plot(x_train, y_train, 'ks', alpha=0.5, label='(x, y)')\n",
    "ax.plot(inputs, outputs[0].T, 'r', lw=2, alpha=0.5, label='posterior draws')\n",
    "ax.plot(inputs, outputs[1:].T, 'r', lw=2, alpha=0.5)\n",
    "ax.set_xlim([-5, 5])\n",
    "ax.set_ylim([-2, 2])\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model has captured the cosine relationship between $x$ and $y$ in the observed domain.\n",
    "\n",
    "\n",
    "To learn more about Edward, [delve in](http://edwardlib.org/api)!\n",
    "\n",
    "If you prefer to learn via examples, then check out some\n",
    "[tutorials](http://edwardlib.org/tutorials/)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
