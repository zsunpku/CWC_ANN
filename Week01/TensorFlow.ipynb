{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seminar - ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Introduction to Neural Networks and TensorFlow\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Graphs -- a computational graph in TensorFlow is a series of operations arranged into a graph of nodes.\n",
    "\n",
    "    The development of Neural Networks algorithms in TensorFlow is done through two distinct steps: 1. building the computational graph and 2. running the computational graph.\n",
    "    Step 1. focuses on creating and defining the nodes of the graph, while step 2. focuses on evaluating the graph through what is called a session.\n",
    "\n",
    "2. Variables -- multiple type of variables exist in TensorFlow -- the most common ones are presneted below:\n",
    "\n",
    "    Constant variables: float or int variables that will remain constant and that we wish to declare in step1\n",
    "    Zeros: tensors with a specific shape that are initiated with zeros\n",
    "    Placeholders: tensors for which we will pass a value in the future. We need to specify a shape when declaring placeholders\n",
    "\n",
    "3. Operations -- TensorFlow has built-in functions for basic and complex operations\n",
    "\n",
    "    Addition: tf.add()\n",
    "    Matrix multiplication: tf.matmul()\n",
    "    ...\n",
    "    But also prebuilt functions, optimization methods, network designs, cells (LSTM), etc.\n",
    "    \n",
    "  The TensorFlow documentation can be found at https://www.tensorflow.org/api_docs/python/\n",
    "    \n",
    "Let's look at an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Const:0\", shape=(), dtype=float32)\n",
      "Tensor(\"zeros:0\", shape=(2, 3), dtype=int32)\n",
      "Tensor(\"Placeholder:0\", shape=(2, 3), dtype=float32)\n",
      "Tensor(\"random_normal:0\", shape=(3, 2), dtype=float32)\n",
      "Tensor(\"MatMul:0\", shape=(2, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Step 1: building the computational graph\n",
    "\n",
    "node1 = tf.constant(7.0, dtype = tf.float32) # equivalent to node1 = 7.0\n",
    "node2 = tf.zeros((2,3), dtype = tf.int32) # tensor of shape (2,3) initiated with zeros\n",
    "node3 = tf.placeholder(dtype = tf.float32, shape = (2,3)) # a placeholder is an empty tensor with a specified shape and type for which we will pass values in the future (when evaluating the graph)\n",
    "node4 = tf.random_normal([3, 2], seed=1234)\n",
    "node5 = tf.matmul(node3,node4)\n",
    "\n",
    "print(node1)\n",
    "print(node2)\n",
    "print(node3)\n",
    "print(node4)\n",
    "print(node5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node1:\n",
      " 7.0\n",
      "node2:\n",
      " [[0 0 0]\n",
      " [0 0 0]]\n",
      "node3:\n",
      " [[ 0.  1.  2.]\n",
      " [ 3.  4.  5.]]\n",
      "node4:\n",
      " [[ 0.51340485 -0.25581399]\n",
      " [ 0.65199131  1.39236379]\n",
      " [ 0.37256798  0.20336303]]\n",
      "node5:\n",
      " [[  3.55750608   2.70003891]\n",
      " [ 12.15140343   9.36564636]]\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Running the computational graph\n",
    "\n",
    "sess = tf.Session()\n",
    "x = [[0,1,2],[3,4,5]]\n",
    "print(\"node1:\\n\", sess.run(node1))\n",
    "print(\"node2:\\n\", sess.run(node2))\n",
    "print(\"node3:\\n\", sess.run(node3, feed_dict={node3:x}))\n",
    "print(\"node4:\\n\", sess.run(node4))\n",
    "print(\"node5:\\n\", sess.run(node5, feed_dict={node3:x}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Application 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>output</th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   output  x1  x2  x3\n",
       "0       1   0   0   1\n",
       "1       1   1   0   1\n",
       "2       0   0   0   0\n",
       "3       1   0   1   0\n",
       "4       0   1   1   1\n",
       "5       0   1   1   1\n",
       "6       1   1   0   1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 0: Let's create some data for our first application\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({'x1':np.array([0,1,0,0,1,1,1]),\n",
    "                   'x2':np.array([0,0,0,1,1,1,0]),\n",
    "                   'x3':np.array([1,1,0,0,1,1,1]),\n",
    "                   'output':np.array([1,1,0,1,0,0,1])})\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://camo.githubusercontent.com/269f47b8185a2ca349ead57db511250553fd918b/687474703a2f2f63733233316e2e6769746875622e696f2f6173736574732f6e6e312f6e657572616c5f6e6574322e6a706567\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resetting to default graph -- especially usefull when running multiple sessions\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Declaring parameters / architecture of neural network\n",
    "num_input_features = 3 # represents the number of features in the input data\n",
    "num_hidden_nodes = 4 # the number of nodes used in the 1st (and only) hidden layer of our network\n",
    "num_classes = 1 # the number of features in the output data -- this is equivalent to a regression problem, we are not trying to predict a class but a number, therefore there is only 1 class\n",
    "learning_rate = 0.01 # parameter used in the optimization process\n",
    "seed = 7 # to replicate results\n",
    "\n",
    "# Declaring placeholders for input data and true outputs\n",
    "inputs = tf.placeholder(tf.float32, shape=[None, 3]) # inputs size will be size of dataset * num_input_features\n",
    "true_outputs = tf.placeholder(tf.float32, shape=[None, 1]) # output size will be size of dataset * num_classes\n",
    "\n",
    "# Randomely initializing weights and biases using normal distribution\n",
    "weights = {\n",
    "    'hidden': tf.Variable(tf.random_normal([num_input_features, num_hidden_nodes], seed=seed)),\n",
    "    'output': tf.Variable(tf.random_normal([num_hidden_nodes, num_classes], seed=seed))}\n",
    "\n",
    "biases = {\n",
    "    'hidden': tf.Variable(tf.random_normal([num_hidden_nodes], seed=seed)),\n",
    "    'output': tf.Variable(tf.random_normal([num_classes], seed=seed))}\n",
    "\n",
    "# Computing layer_1 and output layer (this is a single-layer feed forward neural net) with a sigmoid activation function\n",
    "# The introduction of an activation function allows for non-linearity\n",
    "# Layers are simply equal to activation_function(Wx + biases)\n",
    "\n",
    "layer_1 = tf.nn.sigmoid(tf.add(tf.matmul(inputs,weights['hidden']),biases['hidden']))\n",
    "output_layer = tf.nn.sigmoid(tf.add(tf.matmul(layer_1,weights['output']),biases['output']))\n",
    "\n",
    "# Now that the architecture is designed, let's look at the optimization process -- our objective / cost / error function is the mean square error \n",
    "# We use an iterative optimization process, here the Stochastic Gradient Descent Methode that learns at a predefined learning_rate\n",
    "error = tf.subtract(output_layer, true_outputs)\n",
    "mean_square_error = tf.reduce_sum(tf.square(error))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate).minimize(mean_square_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting optimization\n",
      "Iteration: 0 Mean_square_error: 2.26706 \n",
      "Output\n",
      " [[ 0.83837086]\n",
      " [ 0.88519567]\n",
      " [ 0.80585104]\n",
      " [ 0.77517498]\n",
      " [ 0.87023813]\n",
      " [ 0.87023813]\n",
      " [ 0.88519567]]\n",
      "Iteration: 2000 Mean_square_error: 1.40898 \n",
      "Output\n",
      " [[ 0.66873139]\n",
      " [ 0.70373935]\n",
      " [ 0.59839165]\n",
      " [ 0.3687821 ]\n",
      " [ 0.42848191]\n",
      " [ 0.42848191]\n",
      " [ 0.70373935]]\n",
      "Iteration: 4000 Mean_square_error: 1.06147 \n",
      "Output\n",
      " [[ 0.76707333]\n",
      " [ 0.8082152 ]\n",
      " [ 0.41577893]\n",
      " [ 0.26200432]\n",
      " [ 0.32874194]\n",
      " [ 0.32874194]\n",
      " [ 0.8082152 ]]\n",
      "Iteration: 6000 Mean_square_error: 0.797579 \n",
      "Output\n",
      " [[ 0.85751957]\n",
      " [ 0.88369256]\n",
      " [ 0.28574413]\n",
      " [ 0.2936894 ]\n",
      " [ 0.29128924]\n",
      " [ 0.29128924]\n",
      " [ 0.88369256]]\n",
      "Iteration: 8000 Mean_square_error: 0.425461 \n",
      "Output\n",
      " [[ 0.89129716]\n",
      " [ 0.89529473]\n",
      " [ 0.25673971]\n",
      " [ 0.53540641]\n",
      " [ 0.23447354]\n",
      " [ 0.23447354]\n",
      " [ 0.89529473]]\n",
      "Iteration: 10000 Mean_square_error: 0.179643 \n",
      "Output\n",
      " [[ 0.90059078]\n",
      " [ 0.91688555]\n",
      " [ 0.17294322]\n",
      " [ 0.72849256]\n",
      " [ 0.16173878]\n",
      " [ 0.16173878]\n",
      " [ 0.91688555]]\n",
      "Iteration: 12000 Mean_square_error: 0.0973273 \n",
      "Output\n",
      " [[ 0.91530675]\n",
      " [ 0.93472248]\n",
      " [ 0.12067749]\n",
      " [ 0.80990225]\n",
      " [ 0.12436213]\n",
      " [ 0.12436213]\n",
      " [ 0.93472248]]\n",
      "Very cool, we are finished with the optimiztion!\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Running the graph\n",
    "\n",
    "# Creating a session to run the graph\n",
    "sess = tf.Session()\n",
    "\n",
    "# Initializing all variables\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "# Let's limit the number of iterations \n",
    "iter_ = 0\n",
    "mse = 10\n",
    "print(\"Starting optimization\")\n",
    "\n",
    "while iter_ < 10000 or mse > 0.1:\n",
    "    \n",
    "    # Here we are running the optimization using the Stochastic Gradient Descent Methode\n",
    "    _ = sess.run(train, feed_dict={inputs:np.array(df[['x1','x2','x3']]),true_outputs:np.array(df[['output']])})\n",
    "    \n",
    "    # Displaying results every 2000 iterations\n",
    "    if iter_ % 2000 == 0:\n",
    "        # Evaluating the output layer -- what is predicted for each observation\n",
    "        out = sess.run(output_layer, feed_dict={inputs:np.array(df[['x1','x2','x3']])})\n",
    "        \n",
    "        # Evaluating the mean square error\n",
    "        mse = sess.run(mean_square_error, feed_dict={inputs:np.array(df[['x1','x2','x3']]),true_outputs:np.array(df[['output']])})\n",
    "        print(\"Iteration:\",iter_, \"Mean_square_error:\",mse, \"\\nOutput\\n\",out)\n",
    "    \n",
    "    iter_ += 1\n",
    "\n",
    "print(\"Very cool, we are finished with the optimiztion!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Application 2: digit recognition\n",
    "    Code taken from : https://github.com/aymericdamien/TensorFlow-Examples/blob/master/notebooks/3_NeuralNetworks/neural_network_raw.ipynb\n",
    "    \n",
    "    Problem here: we want to have a neural network recognize which digit correspond to which image. Best algorithms do better than the human eye nowadays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://camo.githubusercontent.com/b06741b45df8ffe29c7de999ab2ec4ff6b2965ba/687474703a2f2f6e657572616c6e6574776f726b73616e64646565706c6561726e696e672e636f6d2f696d616765732f6d6e6973745f3130305f6469676974732e706e67\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Don't look at this -- not important\n",
    "from __future__ import print_function\n",
    "\n",
    "# Import MNIST data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([[ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.35294119,  0.57647061,  1.        ,\n",
      "         1.        ,  0.63137257,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.37254903,  0.59215689,  0.97254908,\n",
      "         0.98823535,  0.99215692,  0.99215692,  0.99215692,  0.93725497,\n",
      "         0.18823531,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.37254903,  0.8588236 ,\n",
      "         0.98431379,  0.99215692,  0.66666669,  0.55686277,  0.37647063,\n",
      "         0.99215692,  0.99215692,  0.99215692,  0.89411771,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.16078432,\n",
      "         0.8588236 ,  0.98039222,  0.99215692,  0.66666669,  0.45490199,\n",
      "         0.01960784,  0.        ,  0.18039216,  0.99215692,  0.99215692,\n",
      "         0.68627453,  0.06666667,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.16078432,  0.88235301,  0.99215692,  0.69411767,\n",
      "         0.09803922,  0.01960784,  0.        ,  0.        ,  0.        ,\n",
      "         0.63921571,  0.99215692,  0.67843139,  0.03921569,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.59215689,\n",
      "         0.99215692,  0.67843139,  0.08235294,  0.        ,  0.        ,\n",
      "         0.        ,  0.08627451,  0.72549021,  0.95686281,  0.99215692,\n",
      "         0.35686275,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.92156869,  0.99215692,  0.20000002,\n",
      "         0.        ,  0.        ,  0.        ,  0.3921569 ,  0.87843144,\n",
      "         0.99215692,  0.99215692,  0.99215692,  0.35686275,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.92156869,  0.99215692,  0.39607847,  0.30980393,  0.30980393,\n",
      "         0.71372551,  0.95294124,  0.99215692,  0.99215692,  0.99215692,\n",
      "         0.70588237,  0.0627451 ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.77647066,  0.99215692,\n",
      "         0.99215692,  0.99215692,  0.99215692,  0.91372555,  0.58039218,\n",
      "         0.38823533,  0.99215692,  0.94117653,  0.23529413,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.30588236,  0.73725492,  0.73725492,  0.73725492,\n",
      "         0.57647061,  0.16470589,  0.06666667,  0.6901961 ,  0.99215692,\n",
      "         0.81176478,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.25882354,  0.99215692,  0.99215692,  0.69411767,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.36470589,  0.99215692,\n",
      "         0.94901967,  0.24313727,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.0509804 ,  0.80784321,  0.99215692,  0.43529415,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.36470589,  0.99215692,\n",
      "         0.95686281,  0.23529413,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.79215693,  0.99215692,  0.69411767,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.24705884,  0.91764712,\n",
      "         0.83529419,  0.09803922,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.20000002,  0.92156869,  0.99215692,  0.69803923,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.36078432,  0.99215692,\n",
      "         0.97647065,  0.21568629,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.20784315,  0.92549026,  0.98431379,  0.43137258,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.8705883 ,  0.81960791,\n",
      "         0.33333334,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ]], dtype=float32), array([[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.]]))\n"
     ]
    }
   ],
   "source": [
    "# Let's look at our data\n",
    "print(mnist.train.next_batch(1))\n",
    "# First array is a 28 * 28 vector with the color intensity for each pixel; the second array is the class of the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resetting to default graph -- especially usefull when running multiple sessions\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.1 # how quickly the model will learn in optimization methode\n",
    "num_steps = 500 # max number of iterations\n",
    "batch_size = 128 # the size of the batch fed in a training iteration to the model\n",
    "display_step = 100 # displaying results of the optimization every 100 iterations\n",
    "\n",
    "# Network Parameters\n",
    "n_hidden_1 = 256 # number of neurons in 1st layer\n",
    "n_hidden_2 = 256 # number of neurons in 2nd layer\n",
    "num_input = 784 # MNIST data input (img shape: 28*28) -- equivalent to the number of features in the input dataset\n",
    "num_classes = 10 # MNIST total classes (0-9 digits) -- number of classes in the output data\n",
    "\n",
    "# tf Graph input\n",
    "X = tf.placeholder(\"float\", [None, num_input])\n",
    "Y = tf.placeholder(\"float\", [None, num_classes])\n",
    "\n",
    "# Store layers weight & bias -- initiating weights and biases using a normal distribution\n",
    "weights = {\n",
    "    'h1': tf.Variable(tf.random_normal([num_input, n_hidden_1])),\n",
    "    'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_2, num_classes]))}\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([num_classes]))}\n",
    "\n",
    "# Create model\n",
    "def neural_net(x):\n",
    "    \n",
    "    # Hidden fully connected layer with 256 neurons\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
    "    # Hidden fully connected layer with 256 neurons\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n",
    "    # Output fully connected layer with a neuron for each class\n",
    "    out_layer = tf.matmul(layer_2, weights['out']) + biases['out']\n",
    "    return out_layer\n",
    "\n",
    "# Construct model\n",
    "logits = neural_net(X)\n",
    "\n",
    "# Define loss and optimizer -- here loss function is the cross entropy to which we apply the softmax function\n",
    "# Softmax function is a normalized exponential funciton that transforms logits into a range from 0 to 1 and with sum if logits equal to 1\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=Y)) # reduce mean is simply the mean of all losses\n",
    "# Here using the Adam algorithm for optimization\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "# Evaluate model\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# Initialize the variables (i.e. assign their default value)\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1, Minibatch Loss= 9833.3623, Training Accuracy= 0.328\n",
      "Step 100, Minibatch Loss= 448.1900, Training Accuracy= 0.812\n",
      "Step 200, Minibatch Loss= 120.2658, Training Accuracy= 0.883\n",
      "Step 300, Minibatch Loss= 102.5413, Training Accuracy= 0.852\n",
      "Step 400, Minibatch Loss= 39.5736, Training Accuracy= 0.852\n",
      "Step 500, Minibatch Loss= 41.0318, Training Accuracy= 0.922\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.8589\n"
     ]
    }
   ],
   "source": [
    "# Start training\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    # Run the initializer\n",
    "    sess.run(init)\n",
    "\n",
    "    for step in range(1, num_steps+1):\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "        # Run optimization op (backprop)\n",
    "        sess.run(train_op, feed_dict={X: batch_x, Y: batch_y})\n",
    "        if step % display_step == 0 or step == 1:\n",
    "            # Calculate batch loss and accuracy\n",
    "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: batch_x, Y: batch_y})\n",
    "            print(\"Step \" + str(step) + \", Minibatch Loss= \" + \"{:.4f}\".format(loss) + \", Training Accuracy= \" + \"{:.3f}\".format(acc))\n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "    \n",
    "    # Calculate accuracy for MNIST test images\n",
    "    print(\"Testing Accuracy:\", sess.run(accuracy, feed_dict={X: mnist.test.images,Y: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# THAT'S IT FOR NOW"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
